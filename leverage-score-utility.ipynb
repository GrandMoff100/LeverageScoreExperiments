{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3bd4cf",
   "metadata": {},
   "source": [
    "# Investigating the Impact of Basis Quality on Leverage-Based Sample Selection\n",
    "\n",
    "## Abstract\n",
    "\n",
    "In this work, we study how basis quality influences leverage-based sample selection by comparing three classes of representations on MNIST: raw pixel features, random ReLU features, and convolutional neural networks with identical architectures but different random initializations. For each basis, we compute leverage scores and evaluate their effectiveness by training only on the top-k leverage points and measuring training accuracy as a function of k.\n",
    "\n",
    "## Features\n",
    "\n",
    "We consider three types of feature representations for the MNIST dataset:\n",
    "1. **Raw Pixel Features**: The original pixel values of the images.\n",
    "2. **Random ReLU Features**: Features generated by applying random ReLU transformations to the raw pixel data.\n",
    "3. **Convolutional Neural Network (CNN) Features**: Features extracted from a CNN with the same architecture but different random initializations.\n",
    "    - CNN A1-A5: Randomly initialized weights. Trained to do classification.\n",
    "    - CNN B1-B5: Randomly initialized weights. Trained to do regression.\n",
    "    - CNN C1-C5: Randomly initialized weights. Not trained.\n",
    "\n",
    "## Investigations\n",
    "\n",
    "### Question 1: How do different basises affect leverage-based sample selection?\n",
    "1. **Leverage Score Computation**: For each feature representation, we compute the leverage scores of the training samples.\n",
    "2. **Sample Selection**: We select the top-k samples based on their leverage scores.\n",
    "3. **Model Training**: We train a classifier using only the selected top-k samples.\n",
    "4. **Evaluation**: We evaluate the training accuracy as a function of k, the number of selected samples.\n",
    "\n",
    "### Question 2: Do neural network prioritize the same samples across different random initializations? (In other words, are the leverage scores correlated across different random initializations? Is the idea of leverage score robust to random initialization?)\n",
    "\n",
    "1. **Varying Basis Quality**: We systematically vary the quality of the basis by using different random initializations for the CNNs.\n",
    "2. **Correlation Analysis**: We analyze the correlation of leverage scores across different random initializations to assess the robustness of leverage-based sample selection. (Measure the swap distance/rank correlation between leverage score rankings. Are they similar to each other? To random shuffling?)\n",
    "3. **Sample Overlap**: We measure the overlap in selected samples across different random initializations to understand if certain samples are consistently prioritized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d44360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda\n",
      "loading B2 from mnist-cnn-B2-d33ec18ec317ceb834e70900948cebf5349cd694b8e60c2fa01df671d884c6e9.pth\n",
      "loading A5 from mnist-cnn-A5-281c64ae0e093b5f25c74f563ab8df4816f691dac7192a6080025a1a5c5a5ea6.pth\n",
      "loading C5 from mnist-cnn-C5-c781165b907876e4f528a451dc3f19f050d02430e024720c6d3234531c53d520.pth\n",
      "loading A1 from mnist-cnn-A1-93d77c3a55680289cf891cc6aa9c777a4068913c3d94f6a12ca8e800343c4ed2.pth\n",
      "loading A3 from mnist-cnn-A3-380064bbb68441c4a2d1846554f2eeef13008958975970e7128c168f9ecf3afe.pth\n",
      "loading C2 from mnist-cnn-C2-5f0b018253844c6bc343087e0574368a8b3ba29212822abb485bca5491bc7305.pth\n",
      "loading C1 from mnist-cnn-C1-c837d179d75114e9d075463863a69b3c29d89a6a5b58caf55ff0ca10db357247.pth\n",
      "loading A4 from mnist-cnn-A4-b5afb85682d831c18f3cbb7fe5248165c77495ef8b708cb5c60e640db5cf706f.pth\n",
      "loading C4 from mnist-cnn-C4-b0f6d55cba7a0bf5656a5177ee36bacfceefe3da2d6fc9ea7cb3f60d8ada3e1a.pth\n",
      "loading B3 from mnist-cnn-B3-0a6871e7460a812e7ac48d7db52e642a0bf82cb6848e9c8b20f69cc4590697b8.pth\n",
      "loading B4 from mnist-cnn-B4-fcfe370d0ff6a966c608170f43accb09c409c3f74c0bdf53a45c9885189810a8.pth\n",
      "loading A2 from mnist-cnn-A2-84ffa0eefcbeeb657268917dd8bee109056e2999075e32338726351bb75c7b0b.pth\n",
      "loading C3 from mnist-cnn-C3-3652111a96ee99a79ac7e9e46ee99ce32b8e23ff3049589268ece4248bbc0c84.pth\n",
      "loading B1 from mnist-cnn-B1-3dfbc9921cd728dc8518b56f50277e7b06c5b1274287f0c423fb6d285234c61e.pth\n",
      "loading B5 from mnist-cnn-B5-279b4287f2a445aaaf5e5675d71f7598494df55d3e566aa6c7527516f14788e8.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from mnist_embeddings import MnistConvNet\n",
    "\n",
    "\n",
    "# Random ReLU Features\n",
    "def relu_features(X, features=200) -> torch.Tensor:\n",
    "    N, *_ = X.shape\n",
    "    X = X.reshape(N, -1)\n",
    "    W = torch.randn(X.shape[1], features, device=X.device)\n",
    "    return torch.relu(X @ W) / np.sqrt(features)\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using\", DEVICE)\n",
    "\n",
    "MNIST_TRAIN = MNIST(root=\"~/Desktop/AliasingOperatorExperiments/data\", train=True, download=True)\n",
    "MNIST_TEST = MNIST(root=\"~/Desktop/AliasingOperatorExperiments/data\", train=False, download=True)\n",
    "\n",
    "mnist_X = (\n",
    "    MNIST_TRAIN\n",
    "    .data.float()\n",
    "    .to(DEVICE)\n",
    "    .reshape(-1, 1, 28, 28)\n",
    "    / 255.0\n",
    ")\n",
    "mnist_y = MNIST_TRAIN.targets.to(DEVICE)\n",
    "test_mnist_X = (\n",
    "    MNIST_TEST\n",
    "    .data.float()\n",
    "    .to(DEVICE)\n",
    "    .reshape(-1, 1, 28, 28)\n",
    "    / 255.0\n",
    ")\n",
    "test_mnist_y = MNIST_TEST.targets.to(DEVICE)\n",
    "\n",
    "combined_mnist_X = torch.cat([mnist_X, test_mnist_X], dim=0)\n",
    "combined_mnist_y = torch.cat([mnist_y, test_mnist_y], dim=0)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "BASIS_FUNCTIONS = 200\n",
    "\n",
    "# Embed the testing set (not the training set)\n",
    "def embed_dataset(X, model, device, basics_functions=BASIS_FUNCTIONS):\n",
    "    # Embed the data using the convolutional layers of the network\n",
    "    embeddings = torch.tensor(\n",
    "        np.zeros(\n",
    "            (\n",
    "                X.shape[0],\n",
    "                basics_functions,\n",
    "            )\n",
    "        )\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_start in range(0, X.shape[0], 256):\n",
    "            batch_end = min(batch_start + 256, X.shape[0])\n",
    "            batch = X[batch_start:batch_end].to(device)\n",
    "            batch_embeddings = model.embed(batch)\n",
    "            embeddings[batch_start:batch_end] = batch_embeddings\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def load_model_embed_data(path: str):\n",
    "    state = torch.load(path)\n",
    "    net = MnistConvNet()\n",
    "    net.load_state_dict(state)\n",
    "    net.to(DEVICE)\n",
    "    net.eval()\n",
    "    embeddings = embed_dataset(combined_mnist_X, net, DEVICE, BASIS_FUNCTIONS)\n",
    "    return embeddings\n",
    "\n",
    "# Embeddings\n",
    "LABELS = combined_mnist_y\n",
    "RAW = combined_mnist_X.reshape(-1, 28 * 28)\n",
    "RELU_FEATURES = relu_features(combined_mnist_X, features=RAW.shape[1])\n",
    "CNN_EMBEDDINGS = {}\n",
    "\n",
    "import glob\n",
    "for path in glob.glob(\"mnist-cnn-*.pth\"):\n",
    "    _, __, letter, hsh = path[:-4].split(\"-\")\n",
    "    print(\"loading\", letter, \"from\", path)\n",
    "    CNN_EMBEDDINGS[letter] = load_model_embed_data(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10de27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_leverage_scores(X: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Calculate the leverage scores for the given data matrix X.\n",
    "\n",
    "    Args:\n",
    "        X (torch.Tensor): Data matrix of shape (N, D).\n",
    "    Returns:\n",
    "        torch.Tensor: Leverage scores of shape (N,).\n",
    "    \"\"\"\n",
    "    Q, _ = torch.linalg.qr(X, mode=\"reduced\")\n",
    "    leverage_scores = torch.sum(Q ** 2, dim=1)\n",
    "    return leverage_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f084f9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "names = [\"RAW\", \"RELU_FEATURES\"] + list(CNN_EMBEDDINGS.keys())\n",
    "for name, embedding in zip(names, [RAW, RELU_FEATURES] + list(CNN_EMBEDDINGS.values())):\n",
    "    leverage_scores = calculate_leverage_scores(embedding).cpu().numpy()\n",
    "\n",
    "    # Plot leverage score distribution\n",
    "    # plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(np.sort(leverage_scores)[::-1])\n",
    "    # plt.grid(True)\n",
    "    # plt.title(f\"Leverage Scores for MNIST {name} Embedding\")\n",
    "    # plt.xlabel(\"Sorted Index\")\n",
    "    # plt.ylabel(\"Leverage Score\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # Plot digits with highest leverage scores\n",
    "    fig_grid_width, fig_grid_height = 10, 30\n",
    "    fig = plt.figure(figsize=(fig_grid_width * 2, fig_grid_height * 2))\n",
    "    top_k = fig_grid_width * fig_grid_height\n",
    "    top_indices = np.argsort(leverage_scores)[-top_k:][::-1]\n",
    "    # print(np.sort(top_indices))  # Save these for later!\n",
    "    for i, idx in enumerate(sorted(top_indices, key=lambda x: combined_mnist_y[x])):\n",
    "        ax = fig.add_subplot(fig_grid_height, fig_grid_width, i + 1)\n",
    "        ax.imshow(combined_mnist_X[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "        ax.set_title(f\"Label: {combined_mnist_y[idx].item()} ({leverage_scores[idx]:.4f})\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/leverage_scores_highest_{name}.png\")  \n",
    "    plt.close()\n",
    "\n",
    "    # Plot digits with lowest leverage scores\n",
    "    fig = plt.figure(figsize=(fig_grid_width * 2, fig_grid_height * 2))\n",
    "    bottom_k = fig_grid_width * fig_grid_height\n",
    "    bottom_indices = np.argsort(leverage_scores)[:bottom_k][::-1]\n",
    "    for i, idx in enumerate(sorted(bottom_indices, key=lambda x: combined_mnist_y[x])):\n",
    "        ax = fig.add_subplot(fig_grid_height, fig_grid_width, i + 1)\n",
    "        ax.imshow(combined_mnist_X[idx].cpu().squeeze(), cmap=\"gray\")\n",
    "        ax.set_title(f\"Label: {combined_mnist_y[idx].item()} ({leverage_scores[idx]:.4f})\")\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/leverage_scores_lowest_{name}.png\")\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
